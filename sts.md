# Short lived Credentials with AWS Security Token Service
### Overview
OpenShift can be configured to use temporary credentials for different components with AWS Security Token Service (STS). It enables an authentication flow allowing a component to assume an IAM Role resulting in short-lived credentials. It also automates requesting and refreshing of credentials using an AWS IAM OpenID Connect (OIDC) Identity Provider. OpenShift can sign ServiceAccount tokens trusted by AWS IAM which can be projected into a Pod and used for authentication. The following is a diagram showing how it works.


### Changes in the Credentials Secret with STS
Previously, if we checked the credentials secret, we'd find the following base64 encoded content in the `credentials` key of the `data` field.

```yaml
[default]
aws_access_key_id = <access_key_id>
secret_access_key = <secret_access_key>
```

With STS we have a full-fledged AWS configuration that defines a `role` and `web identity token`

```yaml
[default]
sts_regional_endpoints = regional
role_name = arn:...:role/some-role-name
web_identity_token_file = /path/to/token
```
The token is a projected ServiceAccount into the Pod, and is short lived for an hour after which it is refreshed.


### Steps to in-place migrate an OpenShift Cluster to STS

---
**NOTE**
This is just for developers interested in taking an existing cluster to STS. This is explicitly NOT RECOMMENED OR SUPPORTED.

---

1. Export variables. Update with values appropriate for your environment.
```shell
export CLUSTER_NAME="newstscluster"
export AWS_ACCESS_KEY_ID="<YOUR ACCESS KEY>"
export AWS_SECRET_ACCESS_KEY="<YOUR SECRET ACCESS KEY>"
export AWS_SESSION_TOKEN="<YOUR SESSION TOKEN>"
export KUBECONFIG="<path to kubeconfig>"
```

1. Extract the cluster's ServiceAccount public signing key:
```bash
oc get configmap --namespace openshift-kube-apiserver bound-sa-token-signing-certs --output json | jq --raw-output '.data["service-account-001.pub"]' > serviceaccount-signer.public
```

2. Pretend that `ccoctl` created a key pair by placing the public key where it would have been created:
```bash
mkdir ./${CLUSTER_NAME} ; mv serviceaccount-signer.public ./${CLUSTER_NAME}/serviceaccount-signer.public
```

3. Get `ccoctl` command and copy to /usr/local/bin
```bash
curl -L -o ccoctl-linux.tar.gz -k https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/ccoctl-linux.tar.gz
sudo tar zxf ccoctl-linux.tar.gz -C /usr/local/bin
```

4. Create the AWS IAM Identity provider and the S3 bucket with the OIDC config files:
```bash
/usr/local/bin/ccoctl aws create-identity-provider --output-dir ${CLUSTER_NAME} --name ${CLUSTER_NAME} --region us-east-2
```

5. Save/note the last line from that output which contains the ARN for the IAM Identity provider.
```bash
export IAM_ARN="arn:aws:iam::898120719963:oidc-provider/newstscluster-oidc.s3.us-east-2.amazonaws.com"
```

6. Update the cluster's Authentication CR's spec.serviceAccountIssuer field to put the URL holding the OIDC files:
```bash
cat ${CLUSTER_NAME}/manifests/cluster-authentication-02-config.yaml | awk '/serviceAccountIssuer/ { print $2 }'
oc edit authentication cluster
```

7. Wait for the kube-apiserver pods to be updated with the new config:
```bash
watch "oc get pods -n openshift-kube-apiserver | grep kube-apiserver"
```

8. Wait for the cluster operators to be ready:
```bash
watch oc get co
```

9. Restart all pods (this *will* take a while) in the cluster (because all ServiceAccounts need to be refreshed after updating the serviceAccountIssuer field):
```bash
$ for I in $(oc get ns -o jsonpath='{range .items[*]} {.metadata.name}{"\n"} {end}'); \
      do oc delete pods --all -n $I; \
      sleep 1; \
      done
```

10. Set the CloudCredentials CR's **.spec.credentialsMode** to Manual with: 
```bash
oc edit cloudcredentials cluster
```

11. Get the current version of the cluster:
```bash
export CLUSTER_VERSION=$(oc get clusterversion version -o json | jq -r '.status.desired.version')
```

12. Get the release image for that version:
```bash
export RELEASE_IMAGE=$(oc get clusterversion version -o json | jq -r --arg CLUSTER_VERSION "${CLUSTER_VERSION}" '.status.history[] | select(.version == $CLUSTER_VERSION) | .image')
```

13. Extract CredentialsRequests resources from that release image:
```bash
oc adm release extract --credentials-requests --cloud=aws ${RELEASE_IMAGE} --to ${CLUSTER_NAME}/cred-reqs
```

14. Create IAM Roles for each of the CredentialsRequests from the release image:
```bash
ccoctl aws create-iam-roles --output-dir ./${CLUSTER_NAME}/ --name ${CLUSTER_NAME} --identity-provider-arn ${IAM_ARN} --region us-east-2 --credentials-requests-dir ${CLUSTER_NAME}/cred-reqs
```

15. Apply the Secrets generated by the above command:
```bash
oc apply -f ${CLUSTER_NAME}/manifests/openshift-cloud-credential-operator-cloud-credential-operator-iam-ro-creds-credentials.yaml
oc apply -f ${CLUSTER_NAME}/manifests/openshift-cloud-network-config-controller-cloud-credentials-credentials.yaml
oc apply -f ${CLUSTER_NAME}/manifests/openshift-cluster-csi-drivers-ebs-cloud-credentials-credentials.yaml
oc apply -f ${CLUSTER_NAME}/manifests/openshift-image-registry-installer-cloud-credentials-credentials.yaml
oc apply -f ${CLUSTER_NAME}/manifests/openshift-ingress-operator-cloud-credentials-credentials.yaml
oc apply -f ${CLUSTER_NAME}/manifests/openshift-machine-api-aws-cloud-credentials-credentials.yaml
```

16. At this point the cluster is using STS. 

### Post STS verification

1. Verify that the OpenShift cluster does not have `aws-creds` credentials. The following command should throw a secret not found error:
   ```yaml
   oc get secrets -n kube-system aws-creds
   ```
2. Verify that components are assuming the IAM Role specified in the secret manifests, instead of creds minted by the cloud-credential-operator. The following command should show you the `role` and `web identity token` used by the image registry operator
   ```yaml
   oc get secrets -n openshift-image-registry installer-cloud-credentials -o json | jq -r .data.credentials | base64 -d
   ```
   sample output of the above command
   ```
   [default]
   role_arn = arn:aws:iam::123456789:role/<aws_infra_name>-openshift-image-registry-installer-cloud-credentials
   web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token
   ```

## Install EBS CSI
Deploy yaml from [GIT:openshift/cluster-storage-operator](https://github.com/openshift/cluster-storage-operator/tree/master/assets/csidriveroperators/aws-ebs/standalone)
1. Deploy the Service account
```shell
oc apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aws-ebs-csi-driver-operator
  namespace: openshift-cluster-csi-drivers
EOF
```
2. Deploy CSI Role
```shell
oc apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: aws-ebs-csi-driver-operator-role
  namespace: openshift-cluster-csi-drivers
rules:
- apiGroups:
  - ''
  resources:
  - pods
  - services
  - endpoints
  - persistentvolumeclaims
  - events
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ''
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  verbs:
  - get
  - create
  - update
  - patch
  - delete
EOF
```
3. Apply CSI role binding
```shell
oc apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: aws-ebs-csi-driver-operator-rolebinding
  namespace: openshift-cluster-csi-drivers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: aws-ebs-csi-driver-operator-role
subjects:
- kind: ServiceAccount
  name: aws-ebs-csi-driver-operator
  namespace: openshift-cluster-csi-drivers
EOF
```
4. Create CSI cluster role
```shell
oc apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aws-ebs-csi-driver-operator-clusterrole
rules:
- apiGroups:
  - security.openshift.io
  resourceNames:
  - privileged
  resources:
  - securitycontextconstraints
  verbs:
  - use
- apiGroups:
  - operator.openshift.io
  resources:
  - clustercsidrivers
  verbs:
  - get
  - list
  - watch
  # The Config Observer controller updates the CR's spec
  - update
  - patch
- apiGroups:
  - operator.openshift.io
  resources:
  - clustercsidrivers/status
  verbs:
  - get
  - list
  - watch
  - update
  - patch
- apiGroups:
  - ''
  resourceNames:
  - extension-apiserver-authentication
  - aws-ebs-csi-driver-operator-lock
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterroles
  - clusterrolebindings
  - roles
  - rolebindings
  verbs:
  - watch
  - list
  - get
  - create
  - delete
  - patch
  - update
- apiGroups:
  - ''
  resources:
  - serviceaccounts
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - list
  - create
  - watch
  - delete
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ''
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ''
  resources:
  - secrets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ''
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - delete
  - update
- apiGroups:
  - ''
  resources:
  - persistentvolumes
  verbs:
  - create
  - delete
  - list
  - get
  - watch
  - update
  - patch
- apiGroups:
  - ''
  resources:
  - persistentvolumeclaims
  verbs:
  - get
  - list
  - watch
  - update
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ''
  resources:
  - persistentvolumeclaims/status
  verbs:
  - patch
  - update
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - storage.k8s.io
  resources:
  - volumeattachments
  verbs:
  - get
  - list
  - watch
  - update
  - delete
  - create
  - patch
- apiGroups:
  - storage.k8s.io
  resources:
  - volumeattachments/status
  verbs:
  - patch
- apiGroups:
  - snapshot.storage.k8s.io
  resources:
  - volumesnapshotcontents/status
  verbs:
  - update
  - patch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  - csinodes
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - delete
- apiGroups:
  - '*'
  resources:
  - events
  verbs:
  - get
  - patch
  - create
  - list
  - watch
  - update
  - delete
- apiGroups:
  - snapshot.storage.k8s.io
  resources:
  - volumesnapshotclasses
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - snapshot.storage.k8s.io
  resources:
  - volumesnapshotcontents
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - delete
  - patch
- apiGroups:
  - snapshot.storage.k8s.io
  resources:
  - volumesnapshots
  verbs:
  - get
  - list
  - watch
  - update
  - patch
- apiGroups:
  - snapshot.storage.k8s.io
  resources:
  - volumesnapshots/status
  verbs:
  - patch
- apiGroups:
  - storage.k8s.io
  resources:
  - csidrivers
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - delete
- apiGroups:
  - cloudcredential.openshift.io
  resources:
  - credentialsrequests
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - config.openshift.io
  resources:
  - infrastructures
  - proxies
  verbs:
  - get
  - list
  - watch
# Allow kube-rbac-proxy to create TokenReview to be able to authenticate Prometheus when collecting metrics
- apiGroups:
  - "authentication.k8s.io"
  resources:
  - "tokenreviews"
  verbs:
  - "create"
EOF
```
5. Create cluster role binding
```shell
oc apply -f - <<EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: aws-ebs-csi-driver-operator-clusterrolebinding
subjects:
  - kind: ServiceAccount
    name: aws-ebs-csi-driver-operator
    namespace: openshift-cluster-csi-drivers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: aws-ebs-csi-driver-operator-clusterrole
EOF
```
6. Create AWS config role
```shell
oc apply -f - <<EOF
# Allow AWS EBS CSI driver operator to read CA bundle from openshift-config-managed/kube-cloud-config ConfigMap
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: aws-ebs-csi-driver-operator-aws-config-role
  namespace: openshift-config-managed
rules:
- apiGroups:
  - ''
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
EOF
```
7. Create AWS config role binding
```shell
oc apply -f - <<EOF
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: aws-ebs-csi-driver-operator-aws-config-clusterrolebinding
  namespace: openshift-config-managed
subjects:
  - kind: ServiceAccount
    name: aws-ebs-csi-driver-operator
    namespace: openshift-cluster-csi-drivers
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: aws-ebs-csi-driver-operator-aws-config-role
EOF
```
8. Deploy EBS CSI driver operator
```shell
export DRIVER_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fc456589ebe5fd98cfa33ec36760575a4102b216960f34199480e321e6a1aaa8
export PROVISIONER_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:edfb8b7505e90a55c57463aff8b19078a198713347a73bdee557690bd010953d
export ATTACHER_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:799ec4597f7ca2be25335a05a55fd5f821d142a25cda4f1783241df058e04a2d
export RESIZER_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1e5021213d9cf7dfb1ab6a15d41f392513b4cd4780689878d5cf73bd009a6e45
export SNAPSHOTTER_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:79ef595c5ceb9bbb34841289b5cc19cb4dc058bc964d768fd71fbe9a987c3979
export NODE_DRIVER_REGISTRAR_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f812b59965f61c4f7c25496b390d5505cc346d2f04a6100e73ef3ea110db6314
export LIVENESS_PROBE_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2dc9aea16a5b9f6b495410c4a37ddc7f1203deb387127cedbedbeb0942a31ff2
export KUBE_RBAC_PROXY_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7e43b54411a8ae991ba69f1583cf94b84f0d54f1dcb9061eb47be9e6eb8fd23a
export OPERATOR_IMAGE=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d1aed48f4db427cc351457fca76c20c0d0b68f54195868297455a52ba04d432a

export LOG_LEVEL="2"

oc apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aws-ebs-csi-driver-operator
  namespace: openshift-cluster-csi-drivers
  annotations:
    config.openshift.io/inject-proxy: aws-ebs-csi-driver-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      name: aws-ebs-csi-driver-operator
  strategy: {}
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
      labels:
        name: aws-ebs-csi-driver-operator
    spec:
      containers:
      - args:
        - start
        - -v=${LOG_LEVEL}
        env:
        - name: DRIVER_IMAGE
          value: ${DRIVER_IMAGE}
        - name: PROVISIONER_IMAGE
          value: ${PROVISIONER_IMAGE}
        - name: ATTACHER_IMAGE
          value: ${ATTACHER_IMAGE}
        - name: RESIZER_IMAGE
          value: ${RESIZER_IMAGE}
        - name: SNAPSHOTTER_IMAGE
          value: ${SNAPSHOTTER_IMAGE}
        - name: NODE_DRIVER_REGISTRAR_IMAGE
          value: ${NODE_DRIVER_REGISTRAR_IMAGE}
        - name: LIVENESS_PROBE_IMAGE
          value: ${LIVENESS_PROBE_IMAGE}
        - name: KUBE_RBAC_PROXY_IMAGE
          value: ${KUBE_RBAC_PROXY_IMAGE}
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        image: ${OPERATOR_IMAGE}
        imagePullPolicy: IfNotPresent
        name: aws-ebs-csi-driver-operator
        resources:
          requests:
            memory: 50Mi
            cpu: 10m
      priorityClassName: system-cluster-critical
      serviceAccountName: aws-ebs-csi-driver-operator
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: "NoSchedule"
EOF
```
9. Create **ClusterCSIDriver** for **EBS**
```shell
oc apply -f - <<EOF
apiVersion: operator.openshift.io/v1
kind: "ClusterCSIDriver"
metadata:
  name: "ebs.csi.aws.com"
spec:
  logLevel: Normal
  managementState: Managed
  operatorLogLevel: Normal
EOF
```
At this point the CSI driver is installed but has crash loopback status because it is unable to retrieve the region from the metadata service. We can bypass this by adding an environment variable (**AWS_REGION**) to the aws-ebs-csi-driver-controller deployment
10. Change **ClusterCSIDriver** operator to **Unmanaged**
```shell
oc apply -f - <<EOF
apiVersion: operator.openshift.io/v1
kind: "ClusterCSIDriver"
metadata:
  name: "ebs.csi.aws.com"
spec:
  logLevel: Normal
  managementState: Unmanaged
  operatorLogLevel: Normal
EOF
```
11. Add environment to deployment
```shell
oc -n openshift-cluster-csi-drivers set env Deployment/aws-ebs-csi-driver-controller AWS_REGION=us-east-2
``` 
12. Verify pods are running
```shell
oc -n openshift-cluster-csi-drivers get po -l app=aws-ebs-csi-driver-controller
```
### Post EBS verification

1. Verify that the Storage Classes are created:
```shell
oc get StorageClass
NAME                PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2-csi             ebs.csi.aws.com   Delete          WaitForFirstConsumer   true                   18h
gp3-csi (default)   ebs.csi.aws.com   Delete          WaitForFirstConsumer   true                   18h
```
2. Verify that you can use the **gp2-csi/gp3-csi** storage classes to create storage.

## Install EFS CSI
1. Create a new credentials request for EFS
   ```shell
   vi ${CLUSTER_NAME}/cred-reqs/0000_50_cluster-efs-csi-driver_credentials_request_aws.yaml 
   ```
   Paste the following content
   ```yaml
   apiVersion: cloudcredential.openshift.io/v1
   kind: CredentialsRequest
   metadata:
   name: openshift-aws-efs-csi-driver
   namespace: openshift-cloud-credential-operator
   spec:
   providerSpec:
      apiVersion: cloudcredential.openshift.io/v1
      kind: AWSProviderSpec
      statementEntries:
      - action:
         - elasticfilesystem:*
         effect: Allow
         resource: '*'
   secretRef:
      name: aws-efs-cloud-credentials
      namespace: openshift-cluster-csi-drivers
   serviceAccountNames:
   - aws-efs-csi-driver-operator
   - aws-efs-csi-driver-controller-sa
   
   ```
2. Create IAM Roles for EFS
```shell
ccoctl aws create-iam-roles --output-dir ./${CLUSTER_NAME}/ --name ${CLUSTER_NAME} --identity-provider-arn ${IAM_ARN} --region us-east-2 --credentials-requests-dir ${CLUSTER_NAME}/cred-reqs
```
3. Apply secret for EFS
```shell
oc apply -f ${CLUSTER_NAME}/manifests/openshift-cluster-csi-drivers-aws-efs-cloud-credentials-credentials.yaml
```
4. Install **AWS EFS CSI Driver Operator**
```shell
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: aws-efs-csi-driver-operator
  namespace: openshift-cluster-csi-drivers
spec:
  channel: stable
  installPlanApproval: Automatic
  name: aws-efs-csi-driver-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
```
5. Install **AWS EFS CSI Driver**
```shell
oc apply -f - <<EOF
apiVersion: operator.openshift.io/v1
kind: ClusterCSIDriver
metadata:
    name: efs.csi.aws.com
spec:
  managementState: Managed
EOF
```
6. Wait for both of these commands to return **True**
```shell
oc get ClusterCSIDriver efs.csi.aws.com -o json | jq -r '.status.conditions[] | select(.type == "AWSEFSDriverNodeServiceControllerAvailable") | .status'
oc get ClusterCSIDriver efs.csi.aws.com -o json | jq -r '.status.conditions[] | select(.type == "AWSEFSDriverControllerServiceControllerAvailable") | .status'
```
7. Create EFS storage class
```shell
oc apply -f - <<EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: efs-csi
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap 
  fileSystemId: fs-a123456789 
  directoryPerms: "700" 
  gidRangeStart: "1000" 
  gidRangeEnd: "2000" 
  basePath: "/dynamic_provisioning" 
EOF
```

### Post EFS verification

1. Verify that the Storage Classes are created:
```shell
oc get StorageClass efs-csi
NAME      PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
efs-csi   efs.csi.aws.com   Delete          Immediate           false                  175m
```
2. Verify that you can use the **efs-csi** storage classes to create storage.
